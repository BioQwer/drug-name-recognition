{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Neural Named Entity Recognition\n",
    "\n",
    "In this notebook, you will find an example of training a neural network to solve Named Entity Recognition (NER) task.\n",
    "In most of the cases, NER task can be formulated as: \n",
    "\n",
    "_Given a sequence of tokens (words, and may be punctuation symbols) provide a tag from predefined set of tags for each token in the sequence._\n",
    "\n",
    "For NER task there are some common types of entities which essentially are tags:\n",
    "- persons\n",
    "- locations\n",
    "- organizations\n",
    "- expressions of time\n",
    "- quantities\n",
    "- monetary values \n",
    "\n",
    "Furthermore, to distinguish consequent entities with the same tags BIO tagging scheme is used. \"B\" stands for beginning, \n",
    "\"I\" stands for the continuation of an entity and \"O\" means the absence of entity. Example with dropped punctuation:\n",
    "\n",
    "    Bernhard        B-PER\n",
    "    Riemann         I-PER\n",
    "    Carl            B-PER\n",
    "    Friedrich       I-PER\n",
    "    Gauss           I-PER\n",
    "    and             O\n",
    "    Leonhard        B-PER\n",
    "    Euler           I-PER\n",
    "\n",
    "In the example above PER means person tag, and \"B-\" and \"I-\" are prefixes identifying beginnings and continuations of the entities. Without such prefixes, it is impossible to separate Bernhard Riemann from Carl Friedrich Gauss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data\n",
    "To train the neural network, you need to have a dataset in the following format:\n",
    "\n",
    "    EU B-ORG\n",
    "    rejects O\n",
    "    the O\n",
    "    call O\n",
    "    of O\n",
    "    Germany B-LOC\n",
    "    to O\n",
    "    boycott O\n",
    "    lamb O\n",
    "    from O\n",
    "    Great B-LOC\n",
    "    Britain I-LOC\n",
    "    . O\n",
    "    \n",
    "    China B-LOC\n",
    "    says O\n",
    "    time O\n",
    "    right O\n",
    "    for O\n",
    "    Taiwan B-LOC\n",
    "    talks O\n",
    "    . O\n",
    "\n",
    "    ...\n",
    "\n",
    "The source text is tokenized and tagged. For each token there is a separate tag with BIO markup. Tags are separated from tokens with whitespaces. Sentences are separated with empty lines.\n",
    "\n",
    "The dataset is a text file or a set of text files.\n",
    "The dataset must be split into three partitions: train, test, and validation. The train set is used for training the network, namely adjusting the weights with gradient descent. The validation set is used for monitoring learning progress and early stopping. The test set is used for final estimation of model quality. Typical partitions of train, validation, and test are 80%, 10%, 10% respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CoNLL 2003 dataset\n",
    "Now we download the CoNLL 2003 dataset from our server and assemble the dataset_dict data structure. \n",
    "dataset_dict is dictionary with fields _'train'_, _'test'_, and _'valid'_. In each field there is a list of training samples. Each sample is a pair (sentence_tokens, sentence_tags). And finally sentence_tokens is a list of tokens and sentence_tags is a list of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from http://lnsigo.mipt.ru/export/datasets/conll2003.tar.gz to conll2003/conll2003.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765k/765k [00:00<00:00, 67.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting conll2003/conll2003.tar.gz archive into conll2003/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples (sentences) in train: 14041\n",
      "Number of samples (sentences) in test : 3453\n",
      "Number of samples (sentences) in valid: 3250\n",
      "\n",
      "Here is a first two samples from the train part of the dataset:\n",
      "Sentence 0\n",
      "Tokens: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Tags:   ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "Sentence 1\n",
      "Tokens: ['Peter', 'Blackburn']\n",
      "Tags:   ['B-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "from ner.utils import download_untar\n",
    "\n",
    "\n",
    "conll_tar_url = 'http://lnsigo.mipt.ru/export/datasets/conll2003.tar.gz'\n",
    "download_path = 'conll2003/'\n",
    "download_untar(conll_tar_url, download_path)\n",
    "\n",
    "data_types = ['train', 'test', 'valid']\n",
    "dataset_dict = dict()\n",
    "for data_type in data_types:\n",
    "\n",
    "    with open('conll2003/' + data_type + '.txt') as f:\n",
    "        xy_list = list()\n",
    "        tokens = list()\n",
    "        tags = list()\n",
    "        for line in f:\n",
    "            items = line.split()\n",
    "            if len(items) > 1 and '-DOCSTART-' not in items[0]:\n",
    "                token, tag = items\n",
    "                if token[0].isdigit():\n",
    "                    tokens.append('#')\n",
    "                else:\n",
    "                    tokens.append(token)\n",
    "                tags.append(tag)\n",
    "            elif len(tokens) > 0:\n",
    "                xy_list.append((tokens, tags,))\n",
    "                tokens = list()\n",
    "                tags = list()\n",
    "        dataset_dict[data_type] = xy_list\n",
    "\n",
    "for key in dataset_dict:\n",
    "    print('Number of samples (sentences) in {:<5}: {}'.format(key, len(dataset_dict[key])))\n",
    "\n",
    "print('\\nHere is a first two samples from the train part of the dataset:')\n",
    "first_two_train_samples = dataset_dict['train'][:2]\n",
    "for n, sample in enumerate(first_two_train_samples):\n",
    "    # sample is a tuple of sentence_tokens and sentence_tags\n",
    "    tokens, tags = sample\n",
    "    print('Sentence {}'.format(n))\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Tags:   {}'.format(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus (batch generator)\n",
    "Now we have to create a Corpus instance. Corpus is a dataprovider. It creates vocabularies to map tokens to indices and generate batches. There is an optional parameter embeddings_file_path in the Corpus constructor. So you can provide the model with pre-trained embeddings. The embeddings must be either a FastText bin file or txt file with the following structure:\n",
    "\n",
    "    400000 100\n",
    "    the -0.038194 -0.24487 ...\n",
    "    of -0.1529 -0.24279 ...\n",
    "\n",
    "where the first line contains the total number of tokens and embeddings dimensionality and the rest lines contains tokens and vectors of embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.corpus import Corpus\n",
    "corp = Corpus(dataset_dict, embeddings_file_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "Now we have to create the Neural Network. To do so we use NER class from the network module. The NER constructor takes the following arguments:\n",
    "\n",
    "    token_embeddings_dim - token embeddings dimensionality (must agree with embeddings if they are provided)\n",
    "    char_embeddings_dim - character embeddings dimensionality \n",
    "    use_crf - whether to use Conditional Random Fields on the top (suggested to always use True)\n",
    "    use_capitalization - whethere to include capitalization binary features to the input of the network.\n",
    "                         If True than binary feature indicating whether the word starts with capital letter\n",
    "                         will be concatenated to the word embeddings.\n",
    "    n_filters - list of output feature dimensionality for each layer. For [100, 200] there will be two\n",
    "                layers with 100 and 200 number of units respectively.\n",
    "    embeddings_dropout - whether to use dropout on embeddings\n",
    "    \n",
    "There are special type of argument determinig what type of net to build:\n",
    "    \n",
    "    net_type - could be one of the following 'cnn', 'rnn', and 'cnn_highway'\n",
    "    \n",
    "For each net type there are a number of optional arguments. For convolutional neural networks ('cnn' and 'cnn_highway' net types) there are:\n",
    "\n",
    "    filter_width - width of the convolutional filter (number of tokens under the filter)\n",
    "    use_batch_norm - if True each layer will be provided with batch normalization\n",
    "\n",
    "For 'rnn' net there is\n",
    "    \n",
    "    cell_type - could be lstm or gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: \n",
      "Embeddings 2014025\n",
      "ConvNet 228352\n",
      "Classifier 1290\n",
      "transitions:0 100\n",
      "Total number of parameters equal 2243767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from ner.network import NER\n",
    "\n",
    "model_params = {\"filter_width\": 7,\n",
    "                \"embeddings_dropout\": True,\n",
    "                \"n_filters\": [\n",
    "                    128, 128,\n",
    "                ],\n",
    "                \"token_embeddings_dim\": 100,\n",
    "                \"char_embeddings_dim\": 25,\n",
    "                \"use_batch_norm\": True,\n",
    "                \"use_crf\": True,\n",
    "                \"net_type\": 'cnn',\n",
    "                \"use_capitalization\": True,\n",
    "               }\n",
    "\n",
    "net = NER(corp, **model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training\n",
    "To train the network the following parameters must be specified:\n",
    "\n",
    "    dropout_rate - probability of dropping the hidden state a value from 0 to 1. 0.5 Works well\n",
    "                   in most of the cases\n",
    "    epochs - number of epochs (10 - 100 typical)\n",
    "    learning_rate: learning rate (0.01 - 0.0001 typical)\n",
    "    batch_size: number of samples in the batch (4 - 64 typical)\n",
    "    learning_rate_decay - multiple factor of decreasing learning rate every epoch (1 - 0.5 typical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5811 phrases; correct: 5091.\n",
      "\n",
      "precision:  87.61%; recall:  85.68%; FB1:  86.63\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5858 phrases; correct: 5254.\n",
      "\n",
      "precision:  89.69%; recall:  88.42%; FB1:  89.05\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5853 phrases; correct: 5265.\n",
      "\n",
      "precision:  89.95%; recall:  88.61%; FB1:  89.28\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5929 phrases; correct: 5328.\n",
      "\n",
      "precision:  89.86%; recall:  89.67%; FB1:  89.76\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5857 phrases; correct: 5295.\n",
      "\n",
      "precision:  90.40%; recall:  89.11%; FB1:  89.75\n",
      "\n",
      "\n",
      "Eval on train:\n",
      "processed 217662 tokens with 23499 phrases; found: 23489 phrases; correct: 23444.\n",
      "\n",
      "precision:  99.81%; recall:  99.77%; FB1:  99.79\n",
      "\n",
      "\tLOC: precision:  99.82%; recall:  99.90%; F1:  99.86 7146\n",
      "\n",
      "\tMISC: precision:  99.53%; recall:  99.36%; F1:  99.45 3432\n",
      "\n",
      "\tORG: precision:  99.84%; recall:  99.65%; F1:  99.75 6309\n",
      "\n",
      "\tPER: precision:  99.91%; recall:  99.94%; F1:  99.92 6602\n",
      "\n",
      "\n",
      "Eval on valid:\n",
      "processed 54612 tokens with 5942 phrases; found: 5858 phrases; correct: 5296.\n",
      "\n",
      "precision:  90.41%; recall:  89.13%; FB1:  89.76\n",
      "\n",
      "\tLOC: precision:  93.87%; recall:  92.54%; F1:  93.20 1811\n",
      "\n",
      "\tMISC: precision:  90.66%; recall:  81.02%; F1:  85.57 824\n",
      "\n",
      "\tORG: precision:  86.26%; recall:  84.27%; F1:  85.25 1310\n",
      "\n",
      "\tPER: precision:  89.86%; recall:  93.32%; F1:  91.56 1913\n",
      "\n",
      "\n",
      "Eval on test:\n",
      "processed 49888 tokens with 5648 phrases; found: 5569 phrases; correct: 4708.\n",
      "\n",
      "precision:  84.54%; recall:  83.36%; FB1:  83.94\n",
      "\n",
      "\tLOC: precision:  87.07%; recall:  88.79%; F1:  87.92 1701\n",
      "\n",
      "\tMISC: precision:  78.56%; recall:  73.08%; F1:  75.72 653\n",
      "\n",
      "\tORG: precision:  81.71%; recall:  76.94%; F1:  79.26 1564\n",
      "\n",
      "\tPER: precision:  86.98%; recall:  88.81%; F1:  87.88 1651\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_params = {'dropout_rate': 0.5,\n",
    "                   'epochs': 5,\n",
    "                   'learning_rate': 0.005,\n",
    "                   'batch_size': 8,\n",
    "                   'learning_rate_decay': 0.707}\n",
    "results = net.fit(**learning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
